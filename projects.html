<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Projects </title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Home</div>
<div class="menu-item"><a href="index.html">About&nbsp;me</a></div>
<div class="menu-category">Research</div>
<div class="menu-item"><a href="projects.html" class="current">Projects</a></div>
<div class="menu-item"><a href="publications.html">Publications</a></div>
<div class="menu-category">Teaching</div>
<div class="menu-item"><a href="courses.html">Courses</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Projects </h1>
</div>
<h2>Decentralized high-dimensional statistical learning</h2>
<p>Research in this context pursues the goal of understanding the overall communication, data, and computational trade-offs when solving decentralized (high-dimensional) statistical learning problems. Specific emphasis is put on the design of schemes that avoid complexities or communication costs scaling with the problem's ambient dimension.
</p>
<h3>Decentralized M-estimation: convex problems</h3>
<p><b>Acceleration in Distributed Sparse Regression</b>
</p>
<p>M. Maros, G. Scutari, NeurIPS 2022
</p>
<p><b>DGD2: A Linearly Convergent Distributed Algorithm for High-dimensional statistical recovery</b>
</p>
<p>M. Maros, G. Scutari, NeurIPS 2022
</p>
<p><b>High-dimensional inference over networks: linear convergence and statistical guarantees</b>
</p>
<p>Y. Sun, M. Maros, G. Scutari, G. Cheng, submitted JMLR 2023.
</p>
<p><b>Decentralized algorithms for sparse high-dimensional M-estimation</b>
</p>
<p>M. Maros, G. Scutari, submitted JMLR 2023.
</p>
<p><b>A unified view of decentralized algorithms for sparse linear regression</b>
</p>
<p>M. Maros, G. Scutari, accepted to CAMSAP 2023.
</p>
<h3>Decentralized M-estimation: non-convex problems</h3>
<p><b>Decentralized Matrix Sensing: Statistical Guarantees and Fast Convergence</b>
</p>
<p>M. Maros, G. Scutari, accepted to NeurIPS 2023.
</p>
<h2>Decentralized optimization and applications to Cyber-physical systems </h2>
<p>Many applications naturally yield decentralized yet time-varying optimization problems, meaning some or all problem parameters become obsolete after a certain amount of time has passed. Research in this context pursues the goal of designing schemes that are robust to parameter changes, while providing theoretical guarantees that certify their performance.
</p>
<h3>Time-varying problems</h3>
<p><b>Dynamic Power Allocation for Smart Grids via ADMM</b>
</p>
<p>M. Maros, J. Jaldén, IEEE SPAWC 2018.
</p>
<p><b>ADMM for distributed dynamic beam-forming</b>
</p>
<p>M. Maros, J. Jaldén, IEEE Transactions of Information processing over Networks, 2018.
</p>
<h3>Time-varying networks</h3>
<p><b>A dual linearly converging method for distributed optimization over time-varying undirected graphs</b>
</p>
<p>M. Maros, J. Jaldén, IEEE CDC, 2018.
</p>
<p><b>ECO-PANDA: a computationally economic, geometrically converging optimization method on time-varying undirected graphs</b>
</p>
<p>M. Maros, J. Jaldén, IEEE ICASSP 2019.
</p>
<p><b>A geometrically converging dual method for distributed optimization over time-varying graphs</b>
</p>
<p>M. Maros, J. Jaldén, IEEE Transactions on Automatic Control, 2020.
</p>
</td>
</tr>
</table>
</body>
</html>
